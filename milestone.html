<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Milestone Report - Gale-Shapley</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <h1>Project Proposal</h1>

  <h2>Title</h2>
  <p><strong>Milestone Report</strong><br>
  Rhea Kripalani and Lucy Wang</p>

  <h2>Summary</h2>
  <p>In the past two and a half weeks, we finalized the problem we are trying to parallelize and implemented two methods of parallelization (shared address space among threads and message passing between processors). </p>
  <p>Problem Statement: Given a full preference list for each man and woman (if there are 2*n participants, each participant has n people in their preference list), we want to write a parallelized Gale-Shapley algorithm that will create a stable pairing for each participant. </p>
  <p>We iterated over whether or not to require a full preference list after coming across a GitHub project that generates data for the Gale-Shapley algorithm and allows the user to specify how long the preference lists in the dataset should be. When we used the data generated from this with our preference list size being less than the number of men or women, our algorithm was not able to find a stable match for everyone, so we decided to stick with using full preference lists for now, as that is the way it works in most applications anyway.</p>
  <p>Our original plan was to implement the PII and PII-SC algorithms as defined in this <a href="https://github.com/bryancjones/gale-shapley">poster</a>, but we realized that these algorithms are much more complex than we previously thought. We spent a week trying to implement and debug the PII algorithm, but we couldn’t get it to converge on stable pairings for everyone. To ensure that we don’t spend all our time debugging one algorithm and potentially not have anything to write about for our report, we decided to pivot to a different parallelization of the algorithm and implement it in a shared address space and message passing. Our next goal is to implement Gale-Shapley with CUDA. </p>

  <h2>Updated Schedule</h2>
  <ul>
    <li><strong>Week 1 (March 24 – March 28)</strong>: Finalize problem formulation, implement sequential Gale-Shapley, and set up benchmarking tools.</li>
    <li><strong>Week 2 (March 31 – April 4)</strong>: Tried to implement the PII algorithm, implemented shared address space parallelization, validated correctness.</li>
    <li><strong>Week 3 (April 7 – April 11)</strong>: Generated large data test cases, implemented message passing parallelization, tested on large data test cases.</li>
    <li><strong>Week 4.0 (April 14 – April 16)</strong>: Lucy - Optimize message passing parallelization, Rhea - Write CUDA implementation, look into PII-SC algorithms again.</li>
    <li><strong>Week 4.5 (April 16 – April 18)</strong>: Rhea - Optimize CUDA implementation, Lucy - Profile the performance of all algorithms.</li>
    <li><strong>Week 5.0 (April 21 – April 23)</strong>: Run experiments to compare all algorithms (experiments will be split among us), finalize results.</li>
    <li><strong>Week 5.5 (April 23 – April 25)</strong>: Complete the final report and poster.</li>
    
  </ul>
  
  <h2>Goals and Deliverables</h2>

  <h3>From the Initial Proposal</h3>
  <ul>
    <li>Plan to achieve:</li>
    <ul>
      <li>Sequential Baseline Implementation</li>
      <ul>
        <li>Completed</li>
      </ul>
      <li>Correctness and Convergence Validation</li>
      <ul>
        <li>Completed</li>
      </ul>
      <li>Parallel Implementation using PII and PII-SC Algorithm</li>
      <ul>
        <li>We moved this to hope to achieve.</li>
        <li>Instead of implementing the PII and PII-SC Algorithm, we plan to implement Gale-Shapley using shared address space, message passing, and CUDA</li>
      </ul>
      <li>Performance and Scalability Benchmarking</li>
      <ul>
        <li>This will be done once all implementations have been completed</li>
      </ul>
      <li>Poster and Visualization</li>
      <ul>
        <li>This will be done once we have analyzed our data</li>
      </ul>
    </ul>
    <li>Nice to have</li>
    <ul>
      <li>Explore PII and PII-SC Algorithms</li>
      <ul>
        <li>Once we finish the CUDA algorithm, we can look into whether it would be worth our time to continue working on this algorithm or if we should simply optimize our current algorithms.</li>
      </ul>
      <li>Thread Efficiency under Sublinear Thread Counts</li>
      <ul>
        <li>We are already using thread configurations for significantly lower thread counts. This is more applicable for the PII-SC algorithm.</li>
      </ul>
      <li>Data Sensitivity Experiments</li>
      <ul>We will look into different manipulations of the preference list and their effect on the algorithm. We can vary the length by using partial rankings or by using biased preferences.</ul>
    </ul>
  </ul>
  <p>In summary, we did have to move some of our goals around. Specifically, we moved the PII and PII-SC Algorithm deliverable to the “nice to haves” section. We replaced this deliverable with the shared address space, message passing, and CUDA implementations. We should be able to complete our new set of goals. Depending on how much time the CUDA implementation takes us, we might be able to achieve one of our “nice to haves” deliverables.</p>

  <h3>New Goals</h3>
  <ul>
    <li>Plan to achieve:</li>
    <ul>
      <li>CUDA implementation of Gale-Shapley</li>
      <li>Optimizations of all algorithms</li>
      <li>Performance analysis</li>
    </ul>
    <li>Nice to have:</li>
    <ul>
      <li>Implement PII or PII-SC Algorithm</li>
      <li>Manipulate input data (change the length of preference lists)</li>
    </ul>
  </ul>

  <h2>Poster Sessions</h2>
  <p>Our goal would be to show a graph of the speedups of our different implementations and address the differences and benefits of using each type of parallelization regarding the Gale-Shapley Algorithm. We will also describe how we completed our parallel implementation and discuss potential future implementations or improvements we would have pursued if we had more time.</p>

  <h2>Preliminary Results</h2>
  <p>Sequential Gale-Shapley Algorithm:</p>
  <table border="1">
    <tr>
    <th><strong>sequential</strong></th>
    <th>initialization time</th>
    <th>computation time</th>
    </tr>
    <tr><td>master/100_100</td><td>0.012341196</td><td>0.000221127</td></tr>
    <tr><td>random/100_100</td><td>0.001781225</td><td>0.000025335</td></tr>
    <tr><td>shuffle/100_100</td><td>0.001931884</td><td>0.000029665</td></tr>
    <tr><td>master/500_500</td><td>0.023034474</td><td>0.000691897</td></tr>
    <tr><td>random/500_500</td><td>0.023155006</td><td>0.00072703</td></tr>
    <tr><td>shuffle/500_500</td><td>0.022265412</td><td>0.000586448</td></tr>
    <tr><td>master/1000_1000</td><td>0.085145133</td><td>0.003029481</td></tr>
    <tr><td>random/1000_1000</td><td>0.085594264</td><td>0.002465578</td></tr>
    <tr><td>shuffle/1000_1000</td><td>0.084825035</td><td>0.002196202</td></tr>
    <tr><td>2000_2000</td><td>0.370548774</td><td>0.009627199</td></tr>
  </table>
  <p>We also have the initialization and computation times of the OpenMP shared address space version for all the test cases listed above. For simplicity, we only included data for n = 1000 and n = 2000 test cases.</p>
  <table border="1">
    <tr>
      <th>n=1000</th>
      <th>initialization time</th>
      <th>computation time</th>
    </tr>
    <tr><td>1 Thread</td><td>0.096207367</td><td>0.006100127</td></tr>
    <tr><td>2 Threads</td><td>0.085109435</td><td>0.005300165</td></tr>
    <tr><td>4 Threads</td><td>0.086066042</td><td>0.005146181</td></tr>
    <tr><td>8 Threads</td><td>0.084503596</td><td>0.006054297</td></tr>
  </table>

  <table border="1">
    <tr>
      <th>n=2000</th>
      <th>initialization time</th>
      <th>computation time</th>
    </tr>
    <tr><td>1 Thread</td><td>0.387654479</td><td>0.026912896</td></tr>
    <tr><td>2 Threads</td><td>0.370879461</td><td>0.021700406</td></tr>
    <tr><td>4 Threads</td><td>0.369560469</td><td>0.019854909</td></tr>
    <tr><td>8 Threads</td><td>0.369961981</td><td>0.022176926</td></tr>
  </table>
  <p>Similarly, we have the results for the OpenMPI message passing version with the same test cases.</p>
  <table border="1">
    <tr>
      <th>n=1000</th>
      <th>initialization time</th>
      <th>computation time</th>
    </tr>
    <tr><td>1 Thread</td><td>0.365146032</td><td>0.004490897</td></tr>
    <tr><td>2 Threads</td><td>0.3604769875</td><td>0.003742539</td></tr>
    <tr><td>4 Threads</td><td>0.3132831853</td><td>0.00417506475</td></tr>
    <tr><td>8 Threads</td><td>0.3411113851</td><td>0.006089136625</td></tr>
  </table>

  <table border="1">
    <tr>
      <th>n=2000</th>
      <th>initialization time</th>
      <th>computation time</th>
    </tr>
    <tr><td>1 Thread</td><td>0.646786017</td><td>0.021756559</td></tr>
    <tr><td>2 Threads</td><td>0.64432722</td><td>0.0192913835</td></tr>
    <tr><td>4 Threads</td><td>0.606660147</td><td>0.022002529</td></tr>
    <tr><td>8 Threads</td><td>0.6338493458</td><td>0.03287053375</td></tr>
  </table>
  <p>There is not much speedup from one thread to eight. Instead, our computation time actually increases a little. Additionally, we noticed that the computation time for our parallel algorithms is larger than our sequential algorithm. We believe this is because there is a lot more overhead with the parallel algorithms, and our datasets aren’t big enough to make the parallelization overhead worth it. The difference in times could also be because we use different approaches to the Gale-Shapley algorithm to fit each method of parallelization. We hope to see clearer trends with larger datasets.</p>

  <h2>Concerns</h2>
  <p>Currently, as we try to generate larger datasets to test our implementation, we exceed our disk quota. We have deleted extra files that aren’t useful from our AFS, yet the largest dataset we can test our implementation on is currently n=2000 with a full preference list. This isn’t enough to compare to real applications. For example, the Gale-Shapley algorithm matches medical students to residencies, and there are around 52,000 students who need to be matched. Because we can’t generate such large datasets, we are concerned that we won’t be able to see the benefits of parallelization due to the extra overhead.</p>
  

</body>
</html>
